### 1.原理推导
假设结果是由两个特征导致,那么可以假设拟合的平面是

$$h_θ(x)=θ_0+θ_1x_1+θ_2x_2=\sum_{i=0}^nθ_ix_i=θ^Tx$$

并且由于预测值和实际值之间是存在误差的,那么对于每一个样本来说都有

$$y^{(i)}=θ^Tx^{(i)}+\epsilon^{(i)}$$

并且对于误差$ε$来说来说,每一个误差都是相互独立,并且服从均值为0方差为$δ^2$的*高斯分布*.那么可以知道对于每个独立的误差,它的的概率密度函数为

$$p(ε^{(i)})=\frac{1}{\sqrt{2π}δ}e^{(-\frac{(ε^{(i)}-0)^2}{2δ^2})}$$

然后由前面的式子可以将概率密度函数转换成

$$p(ε^{(i)})=\frac{1}{\sqrt{2π}δ}e^{(-\frac{(y^{(i)}-θ^Tx^{(i)})^2}{2δ^2})}$$

由于概率密度函数和似然函数在数值上是相等的,那么可以得到 $\theta$ 的似然函数为

$$L(\theta)=\prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta)=\prod_{i=1}^m\frac{1}{\sqrt{2π}δ}e^{(-\frac{(y^{(i)}-θ^Tx^{(i)})^2}{2δ^2})}$$

进一步的对于累乘,可以取对数似然进行化简,那么就有

$$\ln{L(\theta)}=\sum_{i=1}^m\ln{\frac{1}{\sqrt{2π}δ}e^{(-\frac{(y^{(i)}-θ^Tx^{(i)})^2}{2δ^2})}}=m\ln{\frac{1}{\sqrt{2π}δ}}-\frac{1}{\delta^2}\cdot\frac{1}{2}\sum_{i=1}^m(y^{(i)}-θ^Tx^{(i)})^2$$

分析式子可知,为了使似然函数值最大,由于前面可以看做常数想,那么就要使式子

$$J(\theta)=\frac{1}{2}\sum_{i=1}^m(y^{(i)}-θ^Tx^{(i)})^2 \tag{最小二乘法}$$

最小,那么可以得到梯度为0的时候 $J(\theta)$ 就取得最小值,既:

$$\nabla_\theta{J(\theta)}=\nabla_\theta[\frac{1}{2}(X\theta-y)^T(X\theta-y)]=X^TX\theta-X^Ty=0$$

得出

$$\theta=(X^TX)^{-1}X^Ty$$

### 2.梯度下降的思想
通过前面的推导可以得知,当知道 $(X^TX)^{-1}$ 的值的时候是可以精确的求出 $\theta$ 的值的,但是在实际应用的场景中, $(X^TX)^{-1}$ 的值并不是一定能求到的(不是所有的矩阵都能求逆).因此在实际的场景中应该将 $\theta=(X^TX)^{-1}X^Ty$ 当做是一种特殊的情况,甚至可以看做是巧合.这个时候就引入了梯度下降的思想,已知

$$J(\theta)=\frac{1}{2}\sum_{i=1}^m(y^{(i)}-θ^Tx^{(i)})^2$$

那么对于第j个特征来说来说就有梯度为

$$\frac{\partial{J(\theta)}}{\partial{\theta_j}}=-\frac{1}{m}\sum_{i=1}^m(y^i-h_\theta(x^i))x_j^i$$

那么沿着梯度下降的方向就有

$$\theta_j^\prime=\theta_j+\frac{1}{m}\sum_{i=1}^m(y^i-h_\theta(x^i))x_j^i \tag{批量梯度下降:考虑全部样本,速度慢,结果精确}$$

$$\theta_j^\prime=\theta_j+(y^i-h_\theta(x^i))x_j^i \tag{随机梯度下降:每次一个样本,速度快,结果看脸}$$

$$\theta_j^\prime=\theta_j+\alpha\frac{1}{n+1}\sum_{k=i}^{i+n}(y^k-h_\theta(x^k))x_j^k \tag{小批量梯度下降:一般用这个}$$
