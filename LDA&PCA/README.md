### 线性判别分析(LDA)

- **用途**:分类任务中的数据降维

- **目的**:找到具有类间最大区分度的坐标成分,就是找到能将数据按类别进行最大区分的维度

- **原理**:将样本投影到低维度的空间中,使得投影后的样本会按类别分成一簇一簇的情况,相同类别的样本在投影后,在投影空间中会离得更近

- **监督性**:LDA是有监督的,计算的是另一类特定的方向

- **投影**:更适合分类的空间
- **区别**:与PCA不同,PCA关注的是方差,LDA关注的是分类

#### 数学原理

高维到低维的投影可用$y=w^Tx$表示,那么目标就是需要求得$w$

首先每类样本的均值为

$$
\mu_i={1\over N_i}\sum_{x\in w_i}x
$$

那么投影之后的均值为

$$
\tilde{\mu_i}={1\over N_i}\sum_{y\in w_i}y={1\over N_i}\sum_{x\in w_i}w^Tx=w^T\mu_i
$$

这样可以得到投影后两类样本中心点均值的距离$J(w)$为

$$
J(w)=|\tilde{\mu_1}-\tilde{\mu_2}|=|w^t(\mu_1-\mu_2)|
$$

这样看来就有了目标函数$J(w)$了,只需要让$J(w)$越大就越好了,但是实际上有可能投影到低维空间上的样本散布很散,导致可能两类样本的均值中心点离得很远,但是两类样本依然有很多重合的地方,那么这个时候就要求各类样本自身散布比较密集,那么就有:

$$
\tilde{s_i}^2=\sum_{y\in w_i}(y-\tilde{\mu_i})^2
$$

越小越好,这样就得到了最终的目标函数$J(w)$就是:

$$
J(w)={{|\tilde{\mu_1}-\tilde{\mu_2}|^2}\over \tilde{s_1}^2+\tilde{s_2}^2}
$$

而目标就是使这个$J(w)$越大越好,那么先对一些量进行转化

将散列值的公式展开可以得到:

$$
\tilde{s_i}^2=\sum_{y\in w_i}(y-\tilde{\mu_i})^2=\sum_{x\in w_i}(w^Tx-w^T\mu_i)^2=\sum_{x\in w_i}w^T(x-\mu_i)(x-\mu_i)^Tw
$$

其中,由于*散布矩阵*为:

$$
s_i=\sum_{x\in w_i}(x-\mu_i)(x-\mu_i)^T
$$

那么上面的式子就变成了$\tilde{s_i}^2=\sum_{x\in w_i}w^Ts_iw$,并且得到了*类内散布矩阵*:$S_w=s_1+s_2$,于是目标函数$J(w)$的分母就变成了:

$$
\tilde{s_1}^2+\tilde{s_2}^2=w^T S_w w
$$

然后将目标函数的分子也展开:

$$
|\tilde{\mu_1}-\tilde{\mu_2}|^2=(w^T\mu_1-w^T\mu_2)^2=w^T(\mu_1-\mu_2)(\mu_1-\mu_2)w=w^T S_B w \\
,S_B称为类间散布矩阵
$$

这样最终的$J(w)$就是

$$
J(w)={{w^T S_B w}\over{w^T S_w w}}
$$

为了求解$w$,我们需要做归一化,将分母的长度限制为1,因为如果不对分子分母做限制的话,如果同时对分子分母做放缩的话,就会有无穷多个解.然后使用拉格朗日乘子法得出

$$
c(w)=w^T S_B w-\lambda(w^T S_w w-1) \\
\Rightarrow {dc\over dw}=2S_B w-2\lambda S_w w=0 \\
\Rightarrow S_B w=\lambda S_w w
$$

然后式子两边都乘上$S_w$的逆,就得到

$$
S_w^{-1}S_B w=\lambda w \\ 
\Rightarrow w就是矩阵S_w^{-1}S_B的特征向量
$$

最终求解出了$w$

### 主成分分析(PCA)

PCA的目的和LCA是一样的,也是为了做降维,只不过PCA是通过方差来提取最有价值的信息,是一个无监督的问题,并且在做降维后数据也是没有可解释性的

#### 前置概念

- **基**:简单的说就是一组线性无关的向量,例如:(0,1)和(1,0)就是二维空间中的一组基.
- **基变换**:就是将由原来的一组基表示的向量通过另外一组基来表示,过程就是将数据与新的基做内积,得到的结果就是在新的基下面的新坐标分量,说白了就是得到了一个维度到另一个维度的投影
- **协方差**:就是表示两个变量的总体误差,如果协方差是正数,那么这两个变量就是正相关,负数就是负相关,如果协方差为0的话,那就表示这两个变量是完全无关的

#### 数学推导

为了做降维,那么需要解决的问题就是,往那个方向(基)去做投影会保留最多的原始信息.于是一个很直观的想法就是让投影后的数据分的更开.那么在数学上就是让投影后的数据方差

$$
Var(a)={1 \over m}\sum_{i=1}^m (a_i-\mu)^2
$$

更小

但是如果单纯是追求方差最小的话,那么得到的所有基都会往方差最小的方向偏向.即,我们第一步找到一个方差最小的基,然后后续找到的所有基,全都会和第一步找到的基近似,那么投影到这些基上面的信息实际上都是是类似的信息,那么实际上我们获得的原始信息就会很少,这就有悖于一开始的方向.

因此,我们实际上找的基应该是相互独立的,即是这些基应该是正交的.那么可以用协方差

$$
Cov(a,b)={1 \over m} \sum_{i=1}^m a_i b_i
$$

来计算.如果两个字段的协方差是0的话,那么这两个字段就是完全独立的

这样我们就找到了需要优化的目标:为了将K维的数据降维到N维,目标是找到N个单位正交基,使原始数据变换到这组基后,各字段两两之间的协方差为0,字段自身的方差最大.通过协方差矩阵:

$$
X=\left[
\begin{matrix}
a_1 & a_2 & \dots & a_m\\
b_1 & b_2 & \dots & b_m
\end{matrix}
\right] \\
\Longrightarrow {1 \over m}XX^T =
\left[
\begin{matrix}
{1 \over m}\sum_{i=1}^m a_i^2 & {1 \over m}\sum_{i=1}^m a_ib_i\\
{1 \over m}\sum_{i=1}^m a_ib_i & {1 \over m}\sum_{i=1}^m b_i^2
\end{matrix}
\right]
$$

可以看到矩阵对角线上就是两个字段的方差,其他元素就是$a$和$b$的协方差.我们的目的就是使得对角线上的值最大,并且其他位置的值为0

然后一个N行N列的实对称矩阵是一定可以找n个单位正交的特征向量:$E=(e_1,e_2,\dots,e_n)$,然后我们一定可以将协方差矩阵做对角化

$$
E^TCE=\left[
\begin{matrix}
\lambda_1 &  &  & \\
  & \lambda_2 &  & \\
  &  & \ddots & \\
  &  &  & \lambda_n
\end{matrix}
\right]
$$

最后根据特征值从大到小将对应的特征向量进行排列,从中选取前N个组成矩阵,乘以原来的数据,得到的结果就是降维后的结果
