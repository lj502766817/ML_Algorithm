### 原理

#### 基本概念

- 决策树模型:从根节点开始一步一步走到叶子结点就是做决策的过程,最终所有的样本会落到叶子节点上,这样既能用来做分类也能用来做回归
- 决策树的组成
  - 根节点:第一个选择点,也可以理解为影响最大的那个决策
  - 非叶子结点与分支:中间过程,即是做完最大决策后面接着的一步一步细化的决策
  - 叶子节点:最终的决策结果

#### 特征切分(节点选择)

选择什么特征来进行数据切割做出决策,就需要一个衡量标准,最符合标准的那个特征就可以拿来做根节点,然后剩下的特征就可以依次用来做后续的切分

#### 衡量标准-熵

- 熵:表示随机变量不确定度的度量.即是物体内部的混乱程度.
- 表示公式: $H(X)=-\sum p_i*\log(p_i),i=1,2,\dots$ 

例子:样本 $A=\{1,2,3,4\}$ ,样本 $B=\{1,1,1,2,2\}$ 

那么 $A$ 的熵等于

$$H(A)=-0.25*log_2(0.25)-0.25*log_2(0.25)-0.25*log_2(0.25)-0.25*log_2(0.25)=2$$

然后 $B$ 的熵等于

$$H(B)=-0.4*log_2(0.4)-0.6*log_2(0.6)=0.97$$

可以看出来 $A$ 的熵比 $B$ 的熵更大,即 $A$ 比 $B$ 更加的混乱

#### 决策树构建流程

设有一批样本 $A=\{a_1,a_2,\dots,a_{10}\}$ ,每个样本有四个特征 $F=\{f_1,f_2,f_3,f_4\}$ ,A自身的熵为1.先尝试用第一个特征 $f_1$ 来进行分类,假如通过特征 $f_1$ 将A分成了三类 $B_1=\{a_1,a_2,a_3\},B_2=\{a_4,a_5,a_6\},B_3=\{a_7,a_8,a_9,a_{10}\}$ ,并且算的 $B_1$ 的熵为0.8, $B_2$ 的熵为0, $B_3$ 的熵为0.9,那么可以求得通过特征 $f_1$ 进行划分后的信息增益是 $1-0.8*{3 \over 10}-0*{3 \over 10}-0.9*{4 \over 10}=0.4$ ,后续可以依次类推求得每个特征的信息增益,这样就可以选用信息增益最大的那个特征作根节点,然后可以依次找出后面的各个分支.

#### 决策树算法的改进

- ID3:信息增益(如果样本有类似id这种分的很细的特征的话,就会导致根节点使用这些特征,但是这些特征并没有什么意义)
- C4.5:信息增益率(解决ID3问题,考虑自身熵,用分割后的集合熵除以分割前的集合熵)
- CART:使用GINI系数来做衡量标准
  - GINI系数: $Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2$ 

#### 连续值的处理

如果某个特征的值是连续值,比如身高,年龄这种,那么可以将数据排序,然后一个值一个值的二分去做,比如10,12,14就可以按11和13去切,实际上就是把连续值做的离散化

#### 决策树的剪枝

如果让决策树无限制的分化扩展,那么最终可以对每个样本做分类,这样就是过拟合了.为了避免这种过拟合的情况,就需要对决策树进行剪枝

- 预剪枝:可以限制深度,叶子节点个数,叶子节点样本数,信息增益量等一边构建决策树一边进行剪枝
- 后剪枝: $C_\alpha(T)=C(T)+\alpha·|T_{leaf}|---(CCP方式)$ 
