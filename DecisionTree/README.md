### 原理

#### 基本概念

- 决策树模型:从根节点开始一步一步走到叶子结点就是做决策的过程,最终所有的样本会落到叶子节点上,这样既能用来做分类也能用来做回归
- 决策树的组成
  - 根节点:第一个选择点,也可以理解为影响最大的那个决策
  - 非叶子结点与分支:中间过程,即是做完最大决策后面接着的一步一步细化的决策
  - 叶子节点:最终的决策结果

#### 特征切分(节点选择)

选择什么特征来进行数据切割做出决策,就需要一个衡量标准,最符合标准的那个特征就可以拿来做根节点,然后剩下的特征就可以依次用来做后续的切分

#### 衡量标准-熵

- 熵:表示随机变量不确定度的度量.即是物体内部的混乱程度.
- 表示公式:$H(X)=-\sum p_i*\log(p_i),i=1,2,\dots$

例子:样本$A=\{1,2,3,4\}$,样本$B=\{1,1,1,2,2\}$

那么$A$的熵等于

$$H(A)=-0.25*log_2(0.25)-0.25*log_2(0.25)-0.25*log_2(0.25)-0.25*log_2(0.25)=2$$

然后$B$的熵等于

$$H(B)=-0.4*log_2(0.4)-0.6*log_2(0.6)=0.97$$

可以看出来$A$的熵比$B$的熵更大,即$A$比$B$更加的混乱

#### 决策树构建流程

设有一批样本$A=\{a_1,a_2,\dots,a_{10}\}$,每个样本有四个特征$F=\{f_1,f_2,f_3,f_4\}$,A自身的熵为1.先尝试用第一个特征$f_1$来进行分类,假如通过特征$f_1$将A分成了三类$B_1=\{a_1,a_2,a_3\},B_2=\{a_4,a_5,a_6\},B_3=\{a_7,a_8,a_9,a_{10}\}$,并且算的$B_1$的熵为0.8,$B_2$的熵为0,$B_3$的熵为0.9,那么可以求得通过特征$f_1$进行划分后的信息增益是$1-0.8*{3 \over 10}-0*{3 \over 10}-0.9*{4 \over 10}=0.4$,后续可以依次类推求得每个特征的信息增益,这样就可以选用信息增益最大的那个特征作根节点,然后可以依次找出后面的各个分支.
