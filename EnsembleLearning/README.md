### 集成学习原理

#### Bagging(并行)

训练多个决策树,最后的结果取每个决策树的总和的均值

##### Bagging模型-随机森林

从样本中随机的选择一批样本(例如:80%的随机样本),在样本里随机的选择一些特征(例如:60%的随机特征),用这些样本以及特征来构建一个树模型,以此类推可以构建多个树模型,并且由于样本和特征是随机选择的,那么构建出来的树都是不一样的,最后求结果的时候就可以取这些树结果的平均值

之所以要进行随机,是因为要保证泛华能力,如果所有的树都一样,那就没有意义了.随机森林通常选择树模型,训练效果随着树模型的数量增加而增加,不过当数模型的数量增加到一定程度时,结果就趋于稳定

随机森林的优势:

- 能够处理维度很高(即有很多个特征)的数据,不用做特征选择
- 训练完成后可以知道哪些特征是重要特征
- 容易做成并行话,速度较快
- 可以进行可视化展示,便于分析

#### Boosting(串行)

从弱学习器开始加强,通过加权来进行训练

##### Boosting模型-AdaBoost,XgBoots

AdaBoost会根据前一次分类的效果来调整数据的权重,如果某个数据在这一次分类的结果是错的,那么就在下一次分类的时候给这个数据更大的权重,让他分对.最后每个分类器就根据各自的准确性来确定各自的权重,最后组合起来.

#### Stacking堆叠模型(极限追求准确性,通常不会用)

直接拿各种分类器来用,根据第一阶段得到的结果,当做输入来进行第二阶段的训练.结果虽然很好,但是时间通常很慢.
